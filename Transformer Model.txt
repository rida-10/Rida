import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
  x = layers.LayerNormalization(epsilon=1e-6)(inputs)
  x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)
  x = layers.Dropout(dropout)(x)
  res = x + inputs
  x = layers.LayerNormalization(epsilon=1e-6)(res)
  x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation="relu")(x)
  x = layers.Dropout(dropout)(x)
  x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
  return x + res
def transformer(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks,mlp_units, dropout=0):  
  inputs = keras.Input(shape=input_shape)
  x = inputs
  for _ in range(num_transformer_blocks):
    x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)
  x = layers.GlobalAveragePooling1D(data_format="channels_first")(x)
  x = layers.Dropout(dropout)(x)
  x = layers.Dense(mlp_units, activation="relu")(x)
  x = layers.Dropout(dropout)(x)
  outputs = layers.Dense(1, activation="sigmoid")(x)
  return keras.Model(inputs, outputs)
input_shape = (100, 128)
head_size = 64
num_heads = 2
ff_dim = 64
num_transformer_blocks = 2
mlp_units = 64
dropout = 0.1
model = transformer(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks,mlp_units, dropout)
model.summary()
